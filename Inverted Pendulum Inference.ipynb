{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyboard\n",
    "from itertools import count\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next State: [ 0.04549076 -0.20544994  0.04736772  0.32751587] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.04138176 -0.01103323  0.05391804  0.05013882] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.04116109  0.18327579  0.05492081 -0.22505693] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.04482661  0.37757155  0.05041968 -0.49992254] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.05237804  0.18177642  0.04042123 -0.19178595] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.05601357 -0.01389977  0.03658551  0.11336919] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.05573557 -0.20952633  0.03885289  0.41736642] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.05154505 -0.40517673  0.04720022  0.7220404 ] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.04344151 -0.21073838  0.06164103  0.4445794 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.03922674 -0.01654025  0.07053261  0.17194644] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.03889594 -0.2125971   0.07397154  0.4860199 ] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.034644   -0.01859265  0.08369194  0.21753684] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.03427215  0.17523938  0.08804268 -0.04761685] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.03777693 -0.02102749  0.08709034  0.2714943 ] Action: 0 Reward: 1.0\n",
      "Next State: [0.03735638 0.17275074 0.09252023 0.00750076] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.0408114   0.36643243  0.09267024 -0.25461704] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.04814005  0.5601174   0.0875779  -0.5166919 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.0593424   0.7539041   0.07724407 -0.78054285] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.07442047  0.947884    0.06163321 -1.0479579 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.09337816  0.75200075  0.04067405 -0.7365823 ] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.10841817  0.94653803  0.0259424  -1.016192  ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.12734893  0.7510799   0.00561856 -0.7154772 ] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.14237054  0.94612366 -0.00869098 -1.0063864 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.161293    0.75111884 -0.02881871 -0.71644527] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.17631538  0.5564074  -0.04314762 -0.43297088] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.18744352  0.7521128  -0.05180703 -0.7389373 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.20248578  0.5577431  -0.06658578 -0.46299833] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.21364065  0.36362222 -0.07584575 -0.19202404] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.2209131   0.5597426  -0.07968622 -0.5076362 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.23210794  0.7558916  -0.08983895 -0.82432896] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.24722578  0.5621058  -0.10632553 -0.56119967] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.25846788  0.3686241  -0.11754952 -0.30381688] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.26584038  0.56520796 -0.12362586 -0.6311378 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.27714452  0.37200803 -0.13624862 -0.37980312] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.2845847   0.17905734 -0.14384468 -0.13299261] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.28816584 -0.01374259 -0.14650454  0.11107583] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.287891   -0.20649458 -0.14428301  0.3541881 ] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.28376108 -0.00964737 -0.13719925  0.01971318] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.28356814  0.18714821 -0.13680498 -0.3129164 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.2873111  -0.00578661 -0.14306332 -0.06631344] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.28719538  0.19106579 -0.14438958 -0.40049273] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.2910167  -0.00174449 -0.15239944 -0.15659009] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.2909818  -0.19439326 -0.15553124  0.08440158] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.28709394 -0.3869834  -0.15384321  0.3242577 ] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.27935427 -0.5796184  -0.14735806  0.56474584] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.26776192 -0.77239907 -0.13606314  0.8076163 ] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.2523139  -0.5757009  -0.11991081  0.4754167 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.2407999  -0.37910792 -0.11040248  0.14747536] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.23321775 -0.5724899  -0.10745297  0.40338996] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.22176795 -0.7659369  -0.09938517  0.66035646] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.20644921 -0.56958264 -0.08617804  0.3381073 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.19505756 -0.37334687 -0.0794159   0.01954097] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.18759063 -0.5672454  -0.07902507  0.28614834] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.17624572 -0.37109056 -0.07330211 -0.03037483] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.1688239  -0.565089   -0.07390961  0.23830977] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.15752213 -0.36899322 -0.06914341 -0.07674045] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.15014225 -0.17295167 -0.07067822 -0.3904121 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.14668323 -0.36700305 -0.07848646 -0.12082384] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.13934316 -0.17084946 -0.08090294 -0.43719977] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.13592617  0.02531888 -0.08964694 -0.7542494 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.13643256  0.22155485 -0.10473192 -1.0737423 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.14086366  0.41789326 -0.12620677 -1.397371  ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.14922151  0.6143385  -0.15415418 -1.7267016 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.16150829  0.42127892 -0.18868822 -1.4856883 ] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.16993387  0.22889061 -0.21840198 -1.2573736 ] Action: 0 Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "for t in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    (next_state, reward, done, truncated, info) = env.step(action)\n",
    "    print(f\"Next State: {next_state} Action: {action} Reward: {reward}\")\n",
    " \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCartPole(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "        self.env._max_episode_steps = 300\n",
    "        self.new_x_limit = 10.0\n",
    "        self.new_theta_limit = 0.7\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "        x, x_dot, theta, theta_dot = obs\n",
    "\n",
    "        if abs(x) > self.new_x_limit or abs(theta) > self.new_theta_limit:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing based on angel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [-0.04851669 -0.1758074   0.0155233   0.26271942] Action: 0 Reward: 1.0\n",
      "State: [-0.05203284  0.01908956  0.02077769 -0.02502714] Action: 1 Reward: 1.0\n",
      "State: [-0.05165105 -0.1763241   0.02027715  0.2741383 ] Action: 0 Reward: 1.0\n",
      "State: [-0.05517753  0.01850275  0.02575991 -0.01208084] Action: 1 Reward: 1.0\n",
      "State: [-0.05480748 -0.17697898  0.0255183   0.28861704] Action: 0 Reward: 1.0\n",
      "State: [-0.05834706  0.01776996  0.03129064  0.00409032] Action: 1 Reward: 1.0\n",
      "State: [-0.05799166  0.21242951  0.03137245 -0.27855814] Action: 1 Reward: 1.0\n",
      "State: [-0.05374307  0.01687438  0.02580128  0.02385221] Action: 0 Reward: 1.0\n",
      "State: [-0.05340558  0.211617    0.02627833 -0.26057976] Action: 1 Reward: 1.0\n",
      "State: [-0.04917324  0.01612996  0.02106673  0.04027449] Action: 0 Reward: 1.0\n",
      "State: [-0.04885064  0.21094358  0.02187222 -0.24568793] Action: 1 Reward: 1.0\n",
      "State: [-0.04463177  0.01551618  0.01695846  0.05381298] Action: 0 Reward: 1.0\n",
      "State: [-0.04432144  0.21039093  0.01803472 -0.23347154] Action: 1 Reward: 1.0\n",
      "State: [-0.04011363  0.01501599  0.01336529  0.06484511] Action: 0 Reward: 1.0\n",
      "State: [-0.03981331  0.20994379  0.01466219 -0.2235912 ] Action: 1 Reward: 1.0\n",
      "State: [-0.03561443  0.01461538  0.01019037  0.07368045] Action: 0 Reward: 1.0\n",
      "State: [-0.03532212  0.20958976  0.01166398 -0.21577002] Action: 1 Reward: 1.0\n",
      "State: [-0.03113033  0.01430303  0.00734858  0.08056929] Action: 0 Reward: 1.0\n",
      "State: [-0.03084427  0.20931888  0.00895996 -0.20978612] Action: 1 Reward: 1.0\n",
      "State: [-0.02665789  0.01406996  0.00476424  0.0857097 ] Action: 0 Reward: 1.0\n",
      "State: [-0.02637649  0.2091233   0.00647844 -0.2054663 ] Action: 1 Reward: 1.0\n",
      "State: [-0.02219402  0.0139093   0.00236911  0.08925319] Action: 0 Reward: 1.0\n",
      "State: [-0.02191584  0.20899722  0.00415417 -0.20268135] Action: 1 Reward: 1.0\n",
      "State: [-0.01773589  0.0138161   0.00010055  0.09130912] Action: 0 Reward: 1.0\n",
      "State: [-0.01745957  0.20893662  0.00192673 -0.20134208] Action: 1 Reward: 1.0\n",
      "State: [-0.01328084  0.01378716 -0.00210011  0.09194802] Action: 0 Reward: 1.0\n",
      "State: [-0.0130051   0.20893915 -0.00026115 -0.20139676] Action: 1 Reward: 1.0\n",
      "State: [-0.00882631  0.01382093 -0.00428909  0.09120378] Action: 0 Reward: 1.0\n",
      "State: [-0.0085499   0.20900409 -0.00246501 -0.20282926] Action: 1 Reward: 1.0\n",
      "State: [-0.00436981  0.01391748 -0.0065216   0.08907505] Action: 0 Reward: 1.0\n",
      "State: [-0.00409146  0.2091323  -0.0047401  -0.20565829] Action: 1 Reward: 1.0\n",
      "State: [ 9.1182243e-05  1.4078457e-02 -8.8532614e-03  8.5525617e-02] Action: 0 Reward: 1.0\n",
      "State: [ 0.00037275  0.2093262  -0.00714275 -0.20993733] Action: 1 Reward: 1.0\n",
      "State: [ 0.00455928  0.01430709 -0.0113415   0.08048391] Action: 0 Reward: 1.0\n",
      "State: [ 0.00484542  0.20958978 -0.00973182 -0.21575561] Action: 1 Reward: 1.0\n",
      "State: [ 0.00903721  0.01460829 -0.01404693  0.07384165] Action: 0 Reward: 1.0\n",
      "State: [ 0.00932938  0.20992878 -0.0125701  -0.22323982] Action: 1 Reward: 1.0\n",
      "State: [ 0.01352795  0.01498872 -0.01703489  0.06545164] Action: 0 Reward: 1.0\n",
      "State: [ 0.01382773  0.2103507  -0.01572586 -0.23255683] Action: 1 Reward: 1.0\n",
      "State: [ 0.01803474  0.01545695 -0.020377    0.05512444] Action: 0 Reward: 1.0\n",
      "State: [ 0.01834388  0.21086507 -0.01927451 -0.2439173 ] Action: 1 Reward: 1.0\n",
      "State: [ 0.02256118  0.01602364 -0.02415285  0.04262419] Action: 0 Reward: 1.0\n",
      "State: [ 0.02288166  0.21148346 -0.02330037 -0.25758028] Action: 1 Reward: 1.0\n",
      "State: [ 0.02711132  0.01670178 -0.02845198  0.02766325] Action: 0 Reward: 1.0\n",
      "State: [ 0.02744536  0.21221995 -0.02789871 -0.27385905] Action: 1 Reward: 1.0\n",
      "State: [ 0.03168976  0.01750695 -0.03337589  0.00989563] Action: 0 Reward: 1.0\n",
      "State: [ 0.0320399   0.21309127 -0.03317798 -0.29312816] Action: 1 Reward: 1.0\n",
      "State: [ 0.03630172  0.01845768 -0.03904054 -0.01109097] Action: 0 Reward: 1.0\n",
      "State: [ 0.03667088 -0.17608327 -0.03926236  0.2690231 ] Action: 0 Reward: 1.0\n",
      "State: [ 0.03314921  0.01957636 -0.0338819  -0.03578034] Action: 1 Reward: 1.0\n",
      "State: [ 0.03354074 -0.17504375 -0.03459751  0.2460229 ] Action: 0 Reward: 1.0\n",
      "State: [ 0.03003986  0.02055482 -0.02967705 -0.05736901] Action: 1 Reward: 1.0\n",
      "State: [ 0.03045096 -0.17412932 -0.03082443  0.22580484] Action: 0 Reward: 1.0\n",
      "State: [ 0.02696837  0.02141929 -0.02630833 -0.07643969] Action: 1 Reward: 1.0\n",
      "State: [ 0.02739676 -0.17331582 -0.02783713  0.20782815] Action: 0 Reward: 1.0\n",
      "State: [ 0.02393044  0.02219289 -0.02368056 -0.09350437] Action: 1 Reward: 1.0\n",
      "State: [ 0.0243743  -0.17258179 -0.02555065  0.1916143 ] Action: 0 Reward: 1.0\n",
      "State: [ 0.02092266  0.02289619 -0.02171837 -0.10901804] Action: 1 Reward: 1.0\n",
      "State: [ 0.02138059 -0.17190792 -0.02389873  0.1767345 ] Action: 0 Reward: 1.0\n",
      "State: [ 0.01794243  0.02354775 -0.02036404 -0.12339082] Action: 1 Reward: 1.0\n",
      "State: [ 0.01841339 -0.17127663 -0.02283185  0.16279852] Action: 0 Reward: 1.0\n",
      "State: [ 0.01498785  0.02416461 -0.01957588 -0.13699889] Action: 1 Reward: 1.0\n",
      "State: [ 0.01547114 -0.17067155 -0.02231586  0.14944442] Action: 0 Reward: 1.0\n",
      "State: [ 0.01205771  0.02476272 -0.01932697 -0.15019433] Action: 1 Reward: 1.0\n",
      "State: [ 0.01255297 -0.17007722 -0.02233086  0.13632923] Action: 0 Reward: 1.0\n",
      "State: [ 0.00915142  0.02535735 -0.01960428 -0.16331425] Action: 1 Reward: 1.0\n",
      "State: [ 0.00965857 -0.16947855 -0.02287056  0.12312014] Action: 0 Reward: 1.0\n",
      "State: [ 0.006269    0.02596346 -0.02040816 -0.17668957] Action: 1 Reward: 1.0\n",
      "State: [ 0.00678827 -0.16886057 -0.02394195  0.10948605] Action: 0 Reward: 1.0\n",
      "State: [ 0.00341106  0.02659613 -0.02175223 -0.19065325] Action: 1 Reward: 1.0\n",
      "State: [ 0.00394298 -0.16820799 -0.02556529  0.09508915] Action: 0 Reward: 1.0\n",
      "State: [ 0.00057882  0.02727088 -0.02366351 -0.2055487 ] Action: 1 Reward: 1.0\n",
      "State: [ 0.00112424 -0.16750483 -0.02777448  0.07957663] Action: 0 Reward: 1.0\n",
      "State: [-0.00222586  0.02800404 -0.02618295 -0.2217382 ] Action: 1 Reward: 1.0\n",
      "State: [-0.00166578 -0.16673407 -0.03061771  0.06257193] Action: 0 Reward: 1.0\n",
      "State: [-0.00500046  0.02881318 -0.02936628 -0.23961167] Action: 1 Reward: 1.0\n",
      "State: [-0.0044242  -0.16587722 -0.03415851  0.04366557] Action: 0 Reward: 1.0\n",
      "State: [-0.00774174  0.02971748 -0.0332852  -0.259596  ] Action: 1 Reward: 1.0\n",
      "State: [-0.00714739 -0.16491388 -0.03847712  0.02240531] Action: 0 Reward: 1.0\n",
      "State: [-0.01044567  0.03073814 -0.03802901 -0.282165  ] Action: 1 Reward: 1.0\n",
      "State: [-0.00983091 -0.16382132 -0.04367231 -0.00171463] Action: 0 Reward: 1.0\n",
      "State: [-0.01310733 -0.3582906  -0.0437066   0.27687562] Action: 0 Reward: 1.0\n",
      "State: [-0.02027314 -0.16257326 -0.03816909 -0.02926573] Action: 1 Reward: 1.0\n",
      "State: [-0.02352461 -0.35712764 -0.03875441  0.2511343 ] Action: 0 Reward: 1.0\n",
      "State: [-0.03066716 -0.16147433 -0.03373172 -0.05351625] Action: 1 Reward: 1.0\n",
      "State: [-0.03389665 -0.35609677 -0.03480205  0.22833611] Action: 0 Reward: 1.0\n",
      "State: [-0.04101858 -0.16049522 -0.03023532 -0.0751183 ] Action: 1 Reward: 1.0\n",
      "State: [-0.04422849 -0.35517097 -0.03173769  0.20787401] Action: 0 Reward: 1.0\n",
      "State: [-0.05133191 -0.1596099  -0.02758021 -0.09464917] Action: 1 Reward: 1.0\n",
      "State: [-0.05452411 -0.35432592 -0.02947319  0.18920623] Action: 0 Reward: 1.0\n",
      "State: [-0.06161062 -0.15879497 -0.02568907 -0.11262667] Action: 1 Reward: 1.0\n",
      "State: [-0.06478652 -0.3535396  -0.0279416   0.17184211] Action: 0 Reward: 1.0\n",
      "State: [-0.07185732 -0.1580291  -0.02450476 -0.12952292] Action: 1 Reward: 1.0\n",
      "State: [-0.0750179  -0.35279158 -0.02709522  0.15532945] Action: 0 Reward: 1.0\n",
      "State: [-0.08207373 -0.15729238 -0.02398863 -0.14577675] Action: 1 Reward: 1.0\n",
      "State: [-0.08521958 -0.35206273 -0.02690416  0.13924274] Action: 0 Reward: 1.0\n",
      "State: [-0.09226083 -0.15656598 -0.02411931 -0.16180514] Action: 1 Reward: 1.0\n",
      "State: [-0.09539215 -0.35133448 -0.02735541  0.12317226] Action: 0 Reward: 1.0\n",
      "State: [-0.10241884 -0.15583152 -0.02489197 -0.17801395] Action: 1 Reward: 1.0\n",
      "State: [-0.10553547 -0.3505886  -0.02845224  0.10671365] Action: 0 Reward: 1.0\n",
      "State: [-0.11254724 -0.15507069 -0.02631797 -0.19480832] Action: 1 Reward: 1.0\n",
      "State: [-0.11564866 -0.3498065  -0.03021414  0.08945763] Action: 0 Reward: 1.0\n",
      "State: [-0.12264479 -0.15426478 -0.02842499 -0.21260266] Action: 1 Reward: 1.0\n",
      "State: [-0.12573008 -0.34896904 -0.03267704  0.07097992] Action: 0 Reward: 1.0\n",
      "State: [-0.13270946 -0.15339422 -0.03125744 -0.23183103] Action: 1 Reward: 1.0\n",
      "State: [-0.13577734 -0.3480559  -0.03589406  0.05083068] Action: 0 Reward: 1.0\n",
      "State: [-0.14273846 -0.15243815 -0.03487745 -0.25295752] Action: 1 Reward: 1.0\n",
      "State: [-0.14578722 -0.34704518 -0.0399366   0.02852358] Action: 0 Reward: 1.0\n",
      "State: [-0.15272813 -0.15137394 -0.03936613 -0.27648747] Action: 1 Reward: 1.0\n",
      "State: [-0.15575561 -0.34591278 -0.04489588  0.00352407] Action: 0 Reward: 1.0\n",
      "State: [-0.16267386 -0.15017669 -0.04482539 -0.3029792 ] Action: 1 Reward: 1.0\n",
      "State: [-0.1656774  -0.3446321  -0.05088498 -0.02476335] Action: 0 Reward: 1.0\n",
      "State: [-0.17257003 -0.53898877 -0.05138024  0.25144058] Action: 0 Reward: 1.0\n",
      "State: [-0.18334982 -0.3431722  -0.04635143 -0.05699578] Action: 1 Reward: 1.0\n",
      "State: [-0.19021326 -0.5376     -0.04749135  0.22071019] Action: 0 Reward: 1.0\n",
      "State: [-0.20096526 -0.3418325  -0.04307714 -0.08656716] Action: 1 Reward: 1.0\n",
      "State: [-0.20780191 -0.5363113  -0.04480849  0.19221973] Action: 0 Reward: 1.0\n",
      "State: [-0.21852814 -0.34057796 -0.04096409 -0.11425489] Action: 1 Reward: 1.0\n",
      "State: [-0.2253397  -0.53508973 -0.04324919  0.16522793] Action: 0 Reward: 1.0\n",
      "State: [-0.23604149 -0.33937624 -0.03994463 -0.14077912] Action: 1 Reward: 1.0\n",
      "State: [-0.24282901 -0.533904   -0.04276022  0.13903917] Action: 0 Reward: 1.0\n",
      "State: [-0.2535071  -0.33819652 -0.03997943 -0.16682138] Action: 1 Reward: 1.0\n",
      "State: [-0.260271   -0.5327241  -0.04331586  0.11298596] Action: 0 Reward: 1.0\n",
      "State: [-0.2709255  -0.33700907 -0.04105614 -0.19304204] Action: 1 Reward: 1.0\n",
      "State: [-0.27766567 -0.5315204  -0.04491698  0.08641196] Action: 0 Reward: 1.0\n",
      "State: [-0.2882961  -0.33578435 -0.04318874 -0.22009715] Action: 1 Reward: 1.0\n",
      "State: [-0.2950118  -0.5302632  -0.04759068  0.05865555] Action: 0 Reward: 1.0\n",
      "State: [-0.30561703 -0.33449233 -0.04641757 -0.24865463] Action: 1 Reward: 1.0\n",
      "State: [-0.31230688 -0.5289217  -0.05139067  0.02903343] Action: 0 Reward: 1.0\n",
      "State: [-0.32288533 -0.33310187 -0.05081    -0.27941072] Action: 1 Reward: 1.0\n",
      "State: [-0.32954738 -0.52746356 -0.05639821 -0.00317614] Action: 0 Reward: 1.0\n",
      "State: [-0.34009662 -0.7217333  -0.05646173  0.27119276] Action: 0 Reward: 1.0\n",
      "State: [-0.3545313  -0.525853   -0.05103788 -0.03875038] Action: 1 Reward: 1.0\n",
      "State: [-0.36504835 -0.7202073  -0.05181289  0.23740287] Action: 0 Reward: 1.0\n",
      "State: [-0.3794525  -0.52438486 -0.04706483 -0.07116275] Action: 1 Reward: 1.0\n",
      "State: [-0.3899402  -0.7188016  -0.04848808  0.20630744] Action: 0 Reward: 1.0\n",
      "State: [-0.40431625 -0.523021   -0.04436193 -0.10126819] Action: 1 Reward: 1.0\n",
      "State: [-0.41477665 -0.71748006 -0.0463873   0.17709525] Action: 0 Reward: 1.0\n",
      "State: [-0.42912626 -0.521726   -0.04284539 -0.12985292] Action: 1 Reward: 1.0\n",
      "State: [-0.43956077 -0.7162088  -0.04544245  0.14901091] Action: 0 Reward: 1.0\n",
      "State: [-0.45388496 -0.5204666  -0.04246223 -0.1576548 ] Action: 1 Reward: 1.0\n",
      "State: [-0.46429428 -0.7149557  -0.04561533  0.12133573] Action: 0 Reward: 1.0\n",
      "State: [-0.4785934  -0.5192109  -0.04318862 -0.1853823 ] Action: 1 Reward: 1.0\n",
      "State: [-0.4889776  -0.71368915 -0.04689626  0.09336955] Action: 0 Reward: 1.0\n",
      "State: [-0.5032514  -0.5179275  -0.04502887 -0.21373253] Action: 1 Reward: 1.0\n",
      "State: [-0.51360995 -0.7123777  -0.04930352  0.06441314] Action: 0 Reward: 1.0\n",
      "State: [-0.5278575  -0.5165848  -0.04801526 -0.24340868] Action: 1 Reward: 1.0\n",
      "State: [-0.5381892  -0.71098924 -0.05288343  0.03375078] Action: 0 Reward: 1.0\n",
      "State: [-0.552409   -0.51515037 -0.05220842 -0.27513734] Action: 1 Reward: 1.0\n",
      "State: [-5.6271201e-01 -7.0949006e-01 -5.7711165e-02  6.3271559e-04] Action: 0 Reward: 1.0\n",
      "State: [-0.5769018  -0.51359    -0.05769851 -0.3096859 ] Action: 1 Reward: 1.0\n",
      "State: [-0.5871736  -0.70784444 -0.06389223 -0.03574306] Action: 0 Reward: 1.0\n",
      "State: [-0.60133046 -0.9019947  -0.06460709  0.23611663] Action: 0 Reward: 1.0\n",
      "State: [-0.6193704  -0.70601207 -0.05988476 -0.0762253 ] Action: 1 Reward: 1.0\n",
      "State: [-0.6334906  -0.90022665 -0.06140926  0.1969788 ] Action: 0 Reward: 1.0\n",
      "State: [-0.65149516 -0.7042826  -0.05746968 -0.11442632] Action: 1 Reward: 1.0\n",
      "State: [-0.6655808  -0.89853597 -0.05975821  0.15958595] Action: 0 Reward: 1.0\n",
      "State: [-0.68355155 -0.7026116  -0.05656649 -0.15133439] Action: 1 Reward: 1.0\n",
      "State: [-0.69760376 -0.8968799  -0.05959318  0.12298023] Action: 0 Reward: 1.0\n",
      "State: [-0.71554136 -0.70095706 -0.05713357 -0.18789239] Action: 1 Reward: 1.0\n",
      "State: [-0.7295605  -0.895217   -0.06089142  0.08623398] Action: 0 Reward: 1.0\n",
      "State: [-0.74746484 -0.6992775  -0.05916674 -0.22502163] Action: 1 Reward: 1.0\n",
      "State: [-0.7614504  -0.8935061  -0.06366718  0.04842662] Action: 0 Reward: 1.0\n",
      "State: [-0.7793205  -0.69753176 -0.06269865 -0.26364484] Action: 1 Reward: 1.0\n",
      "State: [-0.7932711  -0.89170533 -0.06797154  0.00862189] Action: 0 Reward: 1.0\n",
      "State: [-0.81110525 -0.69567776 -0.06779911 -0.30470875] Action: 1 Reward: 1.0\n",
      "State: [-0.8250188  -0.8897713  -0.07389328 -0.03415499] Action: 0 Reward: 1.0\n",
      "State: [-0.8428142  -1.08376    -0.07457638  0.23432927] Action: 0 Reward: 1.0\n",
      "State: [-0.86448944 -0.8876562  -0.06988979 -0.08091407] Action: 1 Reward: 1.0\n",
      "State: [-0.88224256 -1.0817103  -0.07150807  0.18892558] Action: 0 Reward: 1.0\n",
      "State: [-0.9038768  -0.885642   -0.06772956 -0.12543066] Action: 1 Reward: 1.0\n",
      "State: [-0.9215896  -1.0797316  -0.07023817  0.14513892] Action: 0 Reward: 1.0\n",
      "State: [-0.94318426 -0.8836777  -0.0673354  -0.16884981] Action: 1 Reward: 1.0\n",
      "State: [-0.9608578  -1.0777745  -0.0707124   0.10185388] Action: 0 Reward: 1.0\n",
      "State: [-0.9824133  -0.8817141  -0.06867532 -0.21227384] Action: 1 Reward: 1.0\n",
      "State: [-1.0000476  -1.0757904  -0.07292079  0.0579797 ] Action: 0 Reward: 1.0\n",
      "State: [-1.0215634  -0.87970275 -0.0717612  -0.2567898 ] Action: 1 Reward: 1.0\n",
      "State: [-1.0391574  -1.0737307  -0.076897    0.01242336] Action: 0 Reward: 1.0\n",
      "State: [-1.060632   -0.877595   -0.07664853 -0.30349627] Action: 1 Reward: 1.0\n",
      "State: [-1.0781839  -1.0715456  -0.08271845 -0.03593704] Action: 0 Reward: 1.0\n",
      "State: [-1.0996149  -1.26539    -0.0834372   0.22954376] Action: 0 Reward: 1.0\n",
      "State: [-1.1249226  -1.069181   -0.07884632 -0.08824798] Action: 1 Reward: 1.0\n",
      "State: [-1.1463063  -1.2630894  -0.08061128  0.17855424] Action: 0 Reward: 1.0\n",
      "State: [-1.171568   -1.0669119  -0.0770402  -0.13843043] Action: 1 Reward: 1.0\n",
      "State: [-1.1929063  -1.2608508  -0.0798088   0.12898754] Action: 0 Reward: 1.0\n",
      "State: [-1.2181233  -1.0646816  -0.07722905 -0.18776846] Action: 1 Reward: 1.0\n",
      "State: [-1.239417   -1.2586186  -0.08098442  0.0795871 ] Action: 0 Reward: 1.0\n",
      "State: [-1.2645893  -1.0624347  -0.07939268 -0.23750764] Action: 1 Reward: 1.0\n",
      "State: [-1.285838   -1.256338   -0.08414283  0.02911412] Action: 0 Reward: 1.0\n",
      "State: [-1.3109648  -1.0601164  -0.08356055 -0.28888562] Action: 1 Reward: 1.0\n",
      "State: [-1.3321671  -1.2539536  -0.08933826 -0.02368265] Action: 0 Reward: 1.0\n",
      "State: [-1.3572462  -1.4476883  -0.08981191  0.23953   ] Action: 0 Reward: 1.0\n",
      "State: [-1.3862     -1.2514058  -0.08502132 -0.08007636] Action: 1 Reward: 1.0\n",
      "State: [-1.4112281  -1.4452126  -0.08662284  0.18461753] Action: 0 Reward: 1.0\n",
      "State: [-1.4401323  -1.2489649  -0.08293049 -0.13408557] Action: 1 Reward: 1.0\n",
      "State: [-1.4651116 -1.4428071 -0.0856122  0.1313248] Action: 0 Reward: 1.0\n",
      "State: [-1.4939678  -1.2465698  -0.08298571 -0.18709238] Action: 1 Reward: 1.0\n",
      "State: [-1.5188991  -1.4404124  -0.08672755  0.07830117] Action: 0 Reward: 1.0\n",
      "State: [-1.5477074  -1.2441611  -0.08516153 -0.24043527] Action: 1 Reward: 1.0\n",
      "State: [-1.5725906  -1.4379698  -0.08997024  0.02421699] Action: 0 Reward: 1.0\n",
      "State: [-1.60135   -1.2416805 -0.0894859 -0.2954417] Action: 1 Reward: 1.0\n",
      "State: [-1.6261836  -1.4354203  -0.09539473 -0.03226848] Action: 0 Reward: 1.0\n",
      "State: [-1.654892   -1.629054   -0.0960401   0.22885795] Action: 0 Reward: 1.0\n",
      "State: [-1.687473   -1.4327002  -0.09146294 -0.09250793] Action: 1 Reward: 1.0\n",
      "State: [-1.7161272 -1.6264002 -0.0933131  0.1699751] Action: 0 Reward: 1.0\n",
      "State: [-1.7486551  -1.4300752  -0.0899136  -0.15062563] Action: 1 Reward: 1.0\n",
      "State: [-1.7772566  -1.6238022  -0.09292611  0.11239218] Action: 0 Reward: 1.0\n",
      "State: [-1.8097327  -1.4274801  -0.09067827 -0.20810147] Action: 1 Reward: 1.0\n",
      "State: [-1.8382822  -1.6211963  -0.0948403   0.05465613] Action: 0 Reward: 1.0\n",
      "State: [-1.8707062  -1.4248514  -0.09374718 -0.26637885] Action: 1 Reward: 1.0\n",
      "State: [-1.8992032  -1.6185191  -0.09907475 -0.0046743 ] Action: 0 Reward: 1.0\n",
      "State: [-1.9315736  -1.8120909  -0.09916823  0.25517914] Action: 0 Reward: 1.0\n",
      "State: [-1.9678154  -1.6157031  -0.09406465 -0.06706281] Action: 1 Reward: 1.0\n",
      "State: [-2.0001295  -1.8093594  -0.09540591  0.19452317] Action: 0 Reward: 1.0\n",
      "State: [-2.0363166  -1.6130114  -0.09151544 -0.12666628] Action: 1 Reward: 1.0\n",
      "State: [-2.0685768  -1.8067112  -0.09404877  0.13579968] Action: 0 Reward: 1.0\n",
      "State: [-2.104711   -1.6103768  -0.09133278 -0.18501006] Action: 1 Reward: 1.0\n",
      "State: [-2.1369185  -1.8040813  -0.09503298  0.07752118] Action: 0 Reward: 1.0\n",
      "State: [-2.1730003  -1.6077346  -0.09348255 -0.24356768] Action: 1 Reward: 1.0\n",
      "State: [-2.205155   -1.8014055  -0.09835391  0.01822652] Action: 0 Reward: 1.0\n",
      "State: [-2.241183   -1.6050208  -0.09798938 -0.30379653] Action: 1 Reward: 1.0\n",
      "State: [-2.2732835  -1.7986196  -0.10406531 -0.04355412] Action: 0 Reward: 1.0\n",
      "State: [-2.3092558  -1.9921073  -0.10493639  0.21456726] Action: 0 Reward: 1.0\n",
      "State: [-2.349098   -1.7956538  -0.10064504 -0.10928667] Action: 1 Reward: 1.0\n",
      "State: [-2.3850112  -1.9892004  -0.10283078  0.1500242 ] Action: 0 Reward: 1.0\n",
      "State: [-2.4247952  -1.7927676  -0.09983029 -0.17324726] Action: 1 Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "env = CustomCartPole()\n",
    "\n",
    "state, info = env.reset()\n",
    "\n",
    "for t in range(300):\n",
    "\n",
    "    if state[3] < 0:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = 1\n",
    "\n",
    "    (state, reward, done, truncated, info) = env.step(action)\n",
    "    print(f\"State: {state} Action: {action} Reward: {reward}\")\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        hidden_sizes = [256, 128, 64]\n",
    "        layers = []\n",
    "        \n",
    "        layers.append(nn.Linear(n_observations, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_sizes[-1], n_actions))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.unwrapped.theta_threshold_radians = np.deg2rad(45)\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "policy_net.load_state_dict(torch.load(\"cartpole_policy_8.pth\", map_location=torch.device('cpu'), weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m keyboard\u001b[38;5;241m.\u001b[39mis_pressed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     13\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m---> 15\u001b[0m observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m())\n\u001b[0;32m     16\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(observation, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     18\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'item'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.unwrapped.theta_threshold_radians = np.deg2rad(45)\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=\"cpu\").unsqueeze(0)\n",
    "\n",
    "for t in count():\n",
    "    action = policy_net(state).max(1).indices.view(1, 1)\n",
    "\n",
    "    if keyboard.is_pressed(\"left\"):\n",
    "        action = torch.tensor([[0]], dtype=torch.int64)\n",
    "    elif keyboard.is_pressed(\"right\"):\n",
    "        action = torch.tensor([[1]], dtype=torch.int64)\n",
    "\n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    state = torch.tensor(observation, dtype=torch.float32, device=\"cpu\").unsqueeze(0)\n",
    "    \n",
    "    env.render()\n",
    "\n",
    "    if terminated:\n",
    "        print('Iteration: ', t+1, ' steps')\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
