{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyboard\n",
    "from itertools import count\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next State: [ 0.00780858  0.17839824 -0.04811668 -0.27993217] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.01137654  0.37417236 -0.05371533 -0.5873943 ] Action: 1 Reward: 1.0\n",
      "Next State: [ 0.01885999  0.1798422  -0.06546322 -0.31210423] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.02245684 -0.01428901 -0.0717053  -0.04076356] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.02217105 -0.20831338 -0.07252057  0.22846179] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.01800479 -0.40232807 -0.06795134  0.49741656] Action: 0 Reward: 1.0\n",
      "Next State: [ 0.00995823 -0.5964294  -0.058003    0.7679343 ] Action: 0 Reward: 1.0\n",
      "Next State: [-0.00197036 -0.79070693 -0.04264432  1.0418172 ] Action: 0 Reward: 1.0\n",
      "Next State: [-0.0177845  -0.5950453  -0.02180797  0.736058  ] Action: 1 Reward: 1.0\n",
      "Next State: [-0.02968541 -0.78985935 -0.00708681  1.0217985 ] Action: 0 Reward: 1.0\n",
      "Next State: [-0.04548259 -0.98488617  0.01334916  1.3122479 ] Action: 0 Reward: 1.0\n",
      "Next State: [-0.06518032 -0.78993577  0.03959412  1.0237728 ] Action: 1 Reward: 1.0\n",
      "Next State: [-0.08097903 -0.9855621   0.06006957  1.32862   ] Action: 0 Reward: 1.0\n",
      "Next State: [-0.10069028 -0.79124737  0.08664197  1.0553235 ] Action: 1 Reward: 1.0\n",
      "Next State: [-0.11651523 -0.9874042   0.10774844  1.3738959 ] Action: 0 Reward: 1.0\n",
      "Next State: [-0.13626331 -1.1836953   0.13522635  1.6982418 ] Action: 0 Reward: 1.0\n",
      "Next State: [-0.15993722 -0.990367    0.1691912   1.4505322 ] Action: 1 Reward: 1.0\n",
      "Next State: [-0.17974456 -0.79767954  0.19820184  1.2151333 ] Action: 1 Reward: 1.0\n",
      "Next State: [-0.19569814 -0.60558754  0.22250451  0.9905269 ] Action: 1 Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "for t in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    (next_state, reward, done, truncated, info) = env.step(action)\n",
    "    print(f\"Next State: {next_state} Action: {action} Reward: {reward}\")\n",
    " \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCartPole(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "        self.env._max_episode_steps = 300\n",
    "        self.new_x_limit = 10.0\n",
    "        self.new_theta_limit = 0.7\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "        x, x_dot, theta, theta_dot = obs\n",
    "\n",
    "        if abs(x) > self.new_x_limit or abs(theta) > self.new_theta_limit:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing based on angel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [-0.04855463 -0.20277236 -0.01405544  0.28728727] Action: 0 Reward: 1.0\n",
      "State: [-0.05261008 -0.00745282 -0.0083097  -0.00979528] Action: 1 Reward: 1.0\n",
      "State: [-0.05275913 -0.20245463 -0.0085056   0.2802543 ] Action: 0 Reward: 1.0\n",
      "State: [-0.05680823 -0.00721238 -0.00290051 -0.01509909] Action: 1 Reward: 1.0\n",
      "State: [-0.05695247 -0.2022926  -0.0032025   0.2766673 ] Action: 0 Reward: 1.0\n",
      "State: [-0.06099832 -0.00712512  0.00233085 -0.01702399] Action: 1 Reward: 1.0\n",
      "State: [-0.06114083 -0.20228042  0.00199037  0.27639344] Action: 0 Reward: 1.0\n",
      "State: [-0.06518643 -0.00718692  0.00751824 -0.01566106] Action: 1 Reward: 1.0\n",
      "State: [-0.06533018 -0.20241588  0.00720502  0.27938443] Action: 0 Reward: 1.0\n",
      "State: [-0.0693785  -0.00739745  0.01279271 -0.01101736] Action: 1 Reward: 1.0\n",
      "State: [-0.06952644 -0.20270051  0.01257236  0.2856742 ] Action: 0 Reward: 1.0\n",
      "State: [-0.07358045 -0.0077601   0.01828584 -0.00301712] Action: 1 Reward: 1.0\n",
      "State: [-0.07373565 -0.20313945  0.0182255   0.29537866] Action: 0 Reward: 1.0\n",
      "State: [-0.07779844 -0.00828201  0.02413307  0.00849904] Action: 1 Reward: 1.0\n",
      "State: [-0.07796408  0.18648568  0.02430305 -0.276473  ] Action: 1 Reward: 1.0\n",
      "State: [-0.07423437 -0.00897442  0.01877359  0.02377498] Action: 0 Reward: 1.0\n",
      "State: [-0.07441386  0.18587333  0.01924909 -0.26292604] Action: 1 Reward: 1.0\n",
      "State: [-0.07069639 -0.00951801  0.01399057  0.03576557] Action: 0 Reward: 1.0\n",
      "State: [-0.07088675  0.18540055  0.01470588 -0.25247055] Action: 1 Reward: 1.0\n",
      "State: [-0.06717874 -0.00992827  0.00965647  0.04481436] Action: 0 Reward: 1.0\n",
      "State: [-0.06737731  0.18505388  0.01055276 -0.24480629] Action: 1 Reward: 1.0\n",
      "State: [-0.06367623 -0.01021719  0.00565663  0.05118647] Action: 0 Reward: 1.0\n",
      "State: [-0.06388057  0.1848232   0.00668036 -0.23970638] Action: 1 Reward: 1.0\n",
      "State: [-0.06018411 -0.01039354  0.00188624  0.0550762 ] Action: 0 Reward: 1.0\n",
      "State: [-0.06039198  0.18470131  0.00298776 -0.237011  ] Action: 1 Reward: 1.0\n",
      "State: [-0.05669795 -0.0104632  -0.00175246  0.05661285] Action: 0 Reward: 1.0\n",
      "State: [-0.05690722  0.18468384 -0.0006202  -0.23662247] Action: 1 Reward: 1.0\n",
      "State: [-0.05321354 -0.01042925 -0.00535265  0.05586476] Action: 0 Reward: 1.0\n",
      "State: [-0.05342212  0.18476905 -0.00423536 -0.23850216] Action: 1 Reward: 1.0\n",
      "State: [-0.04972674 -0.01029215 -0.0090054   0.05284182] Action: 0 Reward: 1.0\n",
      "State: [-0.04993259  0.18495776 -0.00794856 -0.24266873] Action: 1 Reward: 1.0\n",
      "State: [-0.04623343 -0.01004975 -0.01280194  0.04749645] Action: 0 Reward: 1.0\n",
      "State: [-0.04643442  0.18525341 -0.01185201 -0.24919794] Action: 1 Reward: 1.0\n",
      "State: [-0.04272936 -0.0096973  -0.01683597  0.03972317] Action: 0 Reward: 1.0\n",
      "State: [-0.04292331  0.18566199 -0.0160415  -0.25822377] Action: 1 Reward: 1.0\n",
      "State: [-0.03921007 -0.00922733 -0.02120598  0.02935658] Action: 0 Reward: 1.0\n",
      "State: [-0.03939461  0.18619221 -0.02061885 -0.2699409 ] Action: 1 Reward: 1.0\n",
      "State: [-0.03567077 -0.00862952 -0.02601767  0.01616807] Action: 0 Reward: 1.0\n",
      "State: [-0.03584336  0.1868557  -0.02569431 -0.28460887] Action: 1 Reward: 1.0\n",
      "State: [-0.03210624 -0.00789054 -0.03138648 -0.00013922] Action: 0 Reward: 1.0\n",
      "State: [-0.03226405 -0.20254864 -0.03138927  0.282478  ] Action: 0 Reward: 1.0\n",
      "State: [-0.03631503 -0.00699335 -0.02573971 -0.01993736] Action: 1 Reward: 1.0\n",
      "State: [-0.03645489 -0.20173688 -0.02613845  0.26451454] Action: 0 Reward: 1.0\n",
      "State: [-0.04048963 -0.0062518  -0.02084816 -0.03629675] Action: 1 Reward: 1.0\n",
      "State: [-0.04061467 -0.20106867 -0.0215741   0.24973615] Action: 0 Reward: 1.0\n",
      "State: [-0.04463604 -0.00564538 -0.01657937 -0.04967287] Action: 1 Reward: 1.0\n",
      "State: [-0.04474895 -0.20052573 -0.01757283  0.23773335] Action: 0 Reward: 1.0\n",
      "State: [-0.04875946 -0.00515719 -0.01281817 -0.06044035] Action: 1 Reward: 1.0\n",
      "State: [-0.04886261 -0.20009303 -0.01402697  0.22817095] Action: 0 Reward: 1.0\n",
      "State: [-0.05286447 -0.00477346 -0.00946355 -0.06890337] Action: 1 Reward: 1.0\n",
      "State: [-0.05295994 -0.19975847 -0.01084162  0.2207788 ] Action: 0 Reward: 1.0\n",
      "State: [-0.05695511 -0.00448324 -0.00642605 -0.07530421] Action: 1 Reward: 1.0\n",
      "State: [-0.05704477 -0.19951248 -0.00793213  0.21534437] Action: 0 Reward: 1.0\n",
      "State: [-0.06103502 -0.00427803 -0.00362524 -0.07983011] Action: 1 Reward: 1.0\n",
      "State: [-0.06112058 -0.19934782 -0.00522184  0.21170685] Action: 0 Reward: 1.0\n",
      "State: [-0.06510754 -0.00415161 -0.00098771 -0.08261871] Action: 1 Reward: 1.0\n",
      "State: [-0.06519057 -0.19925939 -0.00264008  0.20975243] Action: 0 Reward: 1.0\n",
      "State: [-0.06917576 -0.00409978  0.00155497 -0.08376214] Action: 1 Reward: 1.0\n",
      "State: [-6.92577511e-02 -1.99243993e-01 -1.20275916e-04  2.09410980e-01] Action: 0 Reward: 1.0\n",
      "State: [-0.07324263 -0.00412032  0.00406794 -0.08330989] Action: 1 Reward: 1.0\n",
      "State: [-0.07332504 -0.19930035  0.00240175  0.2106537 ] Action: 0 Reward: 1.0\n",
      "State: [-0.07731105 -0.00421282  0.00661482 -0.08127064] Action: 1 Reward: 1.0\n",
      "State: [-0.07739531 -0.19942896  0.00498941  0.21349192] Action: 0 Reward: 1.0\n",
      "State: [-0.08138388 -0.0043787   0.00925925 -0.07761295] Action: 1 Reward: 1.0\n",
      "State: [-0.08147146 -0.19963215  0.00770699  0.21797685] Action: 0 Reward: 1.0\n",
      "State: [-0.0854641  -0.00462122  0.01206652 -0.07226503] Action: 1 Reward: 1.0\n",
      "State: [-0.08555652 -0.19991407  0.01062122  0.22420038] Action: 0 Reward: 1.0\n",
      "State: [-0.0895548  -0.00494552  0.01510523 -0.06511337] Action: 1 Reward: 1.0\n",
      "State: [-0.08965372 -0.20028074  0.01380296  0.23229682] Action: 0 Reward: 1.0\n",
      "State: [-0.09365933 -0.00535871  0.0184489  -0.05600043] Action: 1 Reward: 1.0\n",
      "State: [-0.0937665  -0.20074026  0.01732889  0.24244569] Action: 0 Reward: 1.0\n",
      "State: [-0.09778131 -0.00587007  0.0221778  -0.04472127] Action: 1 Reward: 1.0\n",
      "State: [-0.09789871 -0.2013029   0.02128338  0.25487563] Action: 0 Reward: 1.0\n",
      "State: [-0.10192477 -0.0064912   0.02638089 -0.0310189 ] Action: 1 Reward: 1.0\n",
      "State: [-0.1020546  -0.20198134  0.02576051  0.26986936] Action: 0 Reward: 1.0\n",
      "State: [-0.10609422 -0.0072363   0.0311579  -0.01457854] Action: 1 Reward: 1.0\n",
      "State: [-0.10623895 -0.20279092  0.03086633  0.28776994] Action: 0 Reward: 1.0\n",
      "State: [-0.11029477 -0.00812242  0.03662173  0.00497957] Action: 1 Reward: 1.0\n",
      "State: [-0.11045721  0.1864557   0.03672132 -0.27592754] Action: 1 Reward: 1.0\n",
      "State: [-0.1067281  -0.00917039  0.03120277  0.02810742] Action: 0 Reward: 1.0\n",
      "State: [-0.1069115   0.18549052  0.03176492 -0.25456968] Action: 1 Reward: 1.0\n",
      "State: [-0.10320169 -0.01007024  0.02667353  0.04796079] Action: 0 Reward: 1.0\n",
      "State: [-0.1034031   0.18465927  0.02763274 -0.2361885 ] Action: 1 Reward: 1.0\n",
      "State: [-0.09970991 -0.01084634  0.02290897  0.06508097] Action: 0 Reward: 1.0\n",
      "State: [-0.09992684  0.18393978  0.02421059 -0.22028692] Action: 1 Reward: 1.0\n",
      "State: [-0.09624805 -0.01151971  0.01980485  0.07993364] Action: 0 Reward: 1.0\n",
      "State: [-0.09647844  0.18331282  0.02140353 -0.2064356 ] Action: 1 Reward: 1.0\n",
      "State: [-0.09281218 -0.01210856  0.01727481  0.09292148] Action: 0 Reward: 1.0\n",
      "State: [-0.09305435  0.18276158  0.01913324 -0.19426158] Action: 1 Reward: 1.0\n",
      "State: [-0.08939912 -0.01262876  0.01524801  0.10439512] Action: 0 Reward: 1.0\n",
      "State: [-0.0896517   0.18227139  0.01733591 -0.18343838] Action: 1 Reward: 1.0\n",
      "State: [-0.08600627 -0.01309427  0.01366715  0.11466257] Action: 0 Reward: 1.0\n",
      "State: [-0.08626816  0.18182921  0.0159604  -0.17367733] Action: 1 Reward: 1.0\n",
      "State: [-0.08263157 -0.01351748  0.01248685  0.12399756] Action: 0 Reward: 1.0\n",
      "State: [-0.08290192  0.18142337  0.0149668  -0.16471988] Action: 1 Reward: 1.0\n",
      "State: [-0.07927345 -0.0139096   0.0116724   0.13264683] Action: 0 Reward: 1.0\n",
      "State: [-0.07955164  0.18104322  0.01432534 -0.15633088] Action: 1 Reward: 1.0\n",
      "State: [-0.07593078 -0.01428087  0.01119872  0.1408367 ] Action: 0 Reward: 1.0\n",
      "State: [-0.0762164   0.18067892  0.01401546 -0.14829227] Action: 1 Reward: 1.0\n",
      "State: [-0.07260282 -0.0146409   0.01104961  0.14877908] Action: 0 Reward: 1.0\n",
      "State: [-0.07289564  0.1803211   0.01402519 -0.14039753] Action: 1 Reward: 1.0\n",
      "State: [-0.06928921 -0.01499889  0.01121724  0.1566769 ] Action: 0 Reward: 1.0\n",
      "State: [-0.0695892   0.17996067  0.01435078 -0.13244624] Action: 1 Reward: 1.0\n",
      "State: [-0.06598998 -0.01536387  0.01170186  0.16472939] Action: 0 Reward: 1.0\n",
      "State: [-0.06629726  0.17958863  0.01499644 -0.12423908] Action: 1 Reward: 1.0\n",
      "State: [-0.06270549 -0.01574492  0.01251166  0.17313708] Action: 0 Reward: 1.0\n",
      "State: [-0.06302039  0.17919575  0.0159744  -0.1155727 ] Action: 1 Reward: 1.0\n",
      "State: [-0.05943647 -0.01615141  0.01366295  0.18210693] Action: 0 Reward: 1.0\n",
      "State: [-0.0597595   0.1787724   0.01730509 -0.10623471] Action: 1 Reward: 1.0\n",
      "State: [-0.05618405 -0.01659321  0.01518039  0.19185726] Action: 0 Reward: 1.0\n",
      "State: [-0.05651591  0.17830832  0.01901754 -0.09599849] Action: 1 Reward: 1.0\n",
      "State: [-0.05294975 -0.01708096  0.01709757  0.20262326] Action: 0 Reward: 1.0\n",
      "State: [-0.05329137  0.17779236  0.02115003 -0.08461755] Action: 1 Reward: 1.0\n",
      "State: [-0.04973552 -0.01762629  0.01945768  0.2146625 ] Action: 0 Reward: 1.0\n",
      "State: [-0.05008804  0.17721216  0.02375093 -0.07181965] Action: 1 Reward: 1.0\n",
      "State: [-0.0465438  -0.0182421   0.02231454  0.22826117] Action: 0 Reward: 1.0\n",
      "State: [-0.04690864  0.17655398  0.02687976 -0.05730026] Action: 1 Reward: 1.0\n",
      "State: [-0.04337756 -0.01894287  0.02573376  0.24374077] Action: 0 Reward: 1.0\n",
      "State: [-0.04375642  0.17580225  0.03060857 -0.04071529] Action: 1 Reward: 1.0\n",
      "State: [-0.04024038 -0.01974496  0.02979427  0.26146564] Action: 0 Reward: 1.0\n",
      "State: [-0.04063528  0.1749393   0.03502358 -0.02167293] Action: 1 Reward: 1.0\n",
      "State: [-0.03713649 -0.02066696  0.03459012  0.28185135] Action: 0 Reward: 1.0\n",
      "State: [-0.03754983  0.17394497  0.04022715  0.00027556] Action: 1 Reward: 1.0\n",
      "State: [-0.03407093  0.3684676   0.04023266 -0.27944875] Action: 1 Reward: 1.0\n",
      "State: [-0.02670158  0.1727955   0.03464368  0.02564709] Action: 0 Reward: 1.0\n",
      "State: [-0.02324567  0.36740395  0.03515663 -0.2559072 ] Action: 1 Reward: 1.0\n",
      "State: [-0.01589759  0.17179814  0.03003848  0.0476541 ] Action: 0 Reward: 1.0\n",
      "State: [-0.01246163  0.36647677  0.03099156 -0.23540212] Action: 1 Reward: 1.0\n",
      "State: [-0.00513209  0.17092605  0.02628352  0.06689316] Action: 0 Reward: 1.0\n",
      "State: [-0.00171357  0.3656615   0.02762138 -0.21738267] Action: 1 Reward: 1.0\n",
      "State: [0.00559966 0.17015581 0.02327373 0.08388367] Action: 0 Reward: 1.0\n",
      "State: [ 0.00900278  0.36493656  0.02495141 -0.20136644] Action: 1 Reward: 1.0\n",
      "State: [0.01630151 0.1694668  0.02092408 0.09908187] Action: 0 Reward: 1.0\n",
      "State: [ 0.01969084  0.3642827   0.02290571 -0.18692681] Action: 1 Reward: 1.0\n",
      "State: [0.0269765  0.16884066 0.01916718 0.11289307] Action: 0 Reward: 1.0\n",
      "State: [ 0.03035331  0.36368278  0.02142504 -0.17368165] Action: 1 Reward: 1.0\n",
      "State: [0.03762697 0.16826086 0.01795141 0.12568246] Action: 0 Reward: 1.0\n",
      "State: [ 0.04099219  0.3631211   0.02046506 -0.16128339] Action: 1 Reward: 1.0\n",
      "State: [0.04825461 0.16771224 0.01723939 0.13778484] Action: 0 Reward: 1.0\n",
      "State: [ 0.05160885  0.36258307  0.01999508 -0.14940986] Action: 1 Reward: 1.0\n",
      "State: [0.05886051 0.1671806  0.01700689 0.14951342] Action: 0 Reward: 1.0\n",
      "State: [ 0.06220412  0.36205494  0.01999716 -0.13775602] Action: 1 Reward: 1.0\n",
      "State: [0.06944522 0.16665237 0.01724203 0.16116801] Action: 0 Reward: 1.0\n",
      "State: [ 0.07277827  0.3615233   0.02046539 -0.12602602] Action: 1 Reward: 1.0\n",
      "State: [0.08000874 0.16611423 0.01794487 0.17304261] Action: 0 Reward: 1.0\n",
      "State: [ 0.08333102  0.36097482  0.02140573 -0.1139257 ] Action: 1 Reward: 1.0\n",
      "State: [0.09055052 0.16555278 0.01912721 0.18543299] Action: 0 Reward: 1.0\n",
      "State: [ 0.09386157  0.3603959   0.02283587 -0.10115519] Action: 1 Reward: 1.0\n",
      "State: [0.10106949 0.16495426 0.02081277 0.19864401] Action: 0 Reward: 1.0\n",
      "State: [ 0.10436857  0.35977244  0.02478565 -0.08740145] Action: 1 Reward: 1.0\n",
      "State: [0.11156403 0.16430414 0.02303762 0.21299717] Action: 0 Reward: 1.0\n",
      "State: [ 0.1148501   0.35908926  0.02729756 -0.07233053] Action: 1 Reward: 1.0\n",
      "State: [0.12203189 0.16358683 0.02585095 0.22883837] Action: 0 Reward: 1.0\n",
      "State: [ 0.12530363  0.35833     0.03042772 -0.05557945] Action: 1 Reward: 1.0\n",
      "State: [0.13247024 0.16278528 0.02931613 0.24654622] Action: 0 Reward: 1.0\n",
      "State: [ 0.13572593  0.35747653  0.03424706 -0.03674746] Action: 1 Reward: 1.0\n",
      "State: [0.14287546 0.16188063 0.03351211 0.266541  ] Action: 0 Reward: 1.0\n",
      "State: [ 0.14611308  0.35650867  0.03884293 -0.0153864 ] Action: 1 Reward: 1.0\n",
      "State: [0.15324324 0.1608518  0.0385352  0.28929445] Action: 0 Reward: 1.0\n",
      "State: [0.15646029 0.35540366 0.04432109 0.00900986] Action: 1 Reward: 1.0\n",
      "State: [ 0.16356836  0.5498629   0.04450129 -0.2693664 ] Action: 1 Reward: 1.0\n",
      "State: [0.17456561 0.35413507 0.03911396 0.03701374] Action: 0 Reward: 1.0\n",
      "State: [ 0.18164831  0.54867494  0.03985423 -0.24307628] Action: 1 Reward: 1.0\n",
      "State: [0.19262181 0.35300705 0.03499271 0.06190653] Action: 0 Reward: 1.0\n",
      "State: [ 0.19968195  0.5476102   0.03623084 -0.21953383] Action: 1 Reward: 1.0\n",
      "State: [0.21063416 0.35198963 0.03184016 0.08435405] Action: 0 Reward: 1.0\n",
      "State: [ 0.21767396  0.54664105  0.03352724 -0.19811565] Action: 1 Reward: 1.0\n",
      "State: [0.22860678 0.35105598 0.02956493 0.10495222] Action: 0 Reward: 1.0\n",
      "State: [ 0.2356279   0.54574203  0.03166397 -0.17825843] Action: 1 Reward: 1.0\n",
      "State: [0.24654274 0.3501816  0.0280988  0.12424283] Action: 0 Reward: 1.0\n",
      "State: [ 0.25354636  0.54489     0.03058366 -0.15944447] Action: 1 Reward: 1.0\n",
      "State: [0.26444417 0.34934384 0.02739477 0.14272779] Action: 0 Reward: 1.0\n",
      "State: [ 0.27143106  0.544063    0.03024933 -0.14118823] Action: 1 Reward: 1.0\n",
      "State: [0.2823123  0.3485211  0.02742556 0.16088226] Action: 0 Reward: 1.0\n",
      "State: [ 0.28928274  0.54323995  0.03064321 -0.12302399] Action: 1 Reward: 1.0\n",
      "State: [0.30014753 0.34769267 0.02818273 0.17916688] Action: 0 Reward: 1.0\n",
      "State: [ 0.30710137  0.54240024  0.03176607 -0.10449383] Action: 1 Reward: 1.0\n",
      "State: [0.31794938 0.3468378  0.02967619 0.19803949] Action: 0 Reward: 1.0\n",
      "State: [ 0.32488614  0.541523    0.03363698 -0.0851362 ] Action: 1 Reward: 1.0\n",
      "State: [0.3357166  0.3459354  0.03193425 0.2179666 ] Action: 0 Reward: 1.0\n",
      "State: [ 0.3426353   0.54058665  0.03629359 -0.06447431] Action: 1 Reward: 1.0\n",
      "State: [0.35344705 0.34496364 0.0350041  0.23943493] Action: 0 Reward: 1.0\n",
      "State: [ 0.36034632  0.5395685   0.0397928  -0.04200446] Action: 1 Reward: 1.0\n",
      "State: [0.37113768 0.3438992  0.03895271 0.26296315] Action: 0 Reward: 1.0\n",
      "State: [ 0.37801567  0.5384441   0.04421197 -0.01718366] Action: 1 Reward: 1.0\n",
      "State: [0.38878456 0.3427169  0.0438683  0.28911433] Action: 0 Reward: 1.0\n",
      "State: [0.39563888 0.53718674 0.04965059 0.01058337] Action: 1 Reward: 1.0\n",
      "State: [ 0.40638262  0.7315628   0.04986225 -0.2660301 ] Action: 1 Reward: 1.0\n",
      "State: [0.4210139  0.53576595 0.04454165 0.04195356] Action: 0 Reward: 1.0\n",
      "State: [ 0.4317292   0.7302218   0.04538072 -0.23635004] Action: 1 Reward: 1.0\n",
      "State: [0.44633365 0.5344819  0.04065372 0.07029494] Action: 0 Reward: 1.0\n",
      "State: [ 0.45702326  0.7289981   0.04205962 -0.20928945] Action: 1 Reward: 1.0\n",
      "State: [0.47160324 0.5333008  0.03787383 0.09635882] Action: 0 Reward: 1.0\n",
      "State: [ 0.48226926  0.72786003  0.03980101 -0.1841385 ] Action: 1 Reward: 1.0\n",
      "State: [0.49682644 0.5321919  0.03611824 0.12082972] Action: 0 Reward: 1.0\n",
      "State: [ 0.5074703   0.7267782   0.03853483 -0.16024302] Action: 1 Reward: 1.0\n",
      "State: [0.52200586 0.5311264  0.03532997 0.1443433 ] Action: 0 Reward: 1.0\n",
      "State: [ 0.53262836  0.72572505  0.03821684 -0.13698764] Action: 1 Reward: 1.0\n",
      "State: [0.54714286 0.5300771  0.03547709 0.16750294] Action: 0 Reward: 1.0\n",
      "State: [ 0.55774444  0.72467375  0.03882714 -0.11378027] Action: 1 Reward: 1.0\n",
      "State: [0.5722379  0.5290176  0.03655154 0.1908951 ] Action: 0 Reward: 1.0\n",
      "State: [ 0.58281827  0.7235981   0.04036944 -0.09003694] Action: 1 Reward: 1.0\n",
      "State: [0.5972902  0.52792144 0.0385687  0.21510427] Action: 0 Reward: 1.0\n",
      "State: [ 0.60784864  0.7224714   0.04287079 -0.06516723] Action: 1 Reward: 1.0\n",
      "State: [0.62229806 0.5267619  0.04156744 0.24072751] Action: 0 Reward: 1.0\n",
      "State: [ 0.6328333   0.72126615  0.046382   -0.03855957] Action: 1 Reward: 1.0\n",
      "State: [0.64725864 0.5255108  0.0456108  0.26838905] Action: 0 Reward: 1.0\n",
      "State: [ 0.65776885  0.7199532   0.05097858 -0.00956624] Action: 1 Reward: 1.0\n",
      "State: [0.6721679  0.52413857 0.05078726 0.29875526] Action: 0 Reward: 1.0\n",
      "State: [0.6826507  0.7185012  0.05676236 0.02251231] Action: 1 Reward: 1.0\n",
      "State: [ 0.6970207   0.9127651   0.05721261 -0.2517351 ] Action: 1 Reward: 1.0\n",
      "State: [0.715276   0.71687484 0.05217791 0.05843088] Action: 0 Reward: 1.0\n",
      "State: [ 0.7296135   0.9112113   0.05334653 -0.21734379] Action: 1 Reward: 1.0\n",
      "State: [0.7478377  0.715369   0.04899965 0.09167829] Action: 0 Reward: 1.0\n",
      "State: [ 0.7621451   0.9097556   0.05083321 -0.18515155] Action: 1 Reward: 1.0\n",
      "State: [0.7803402  0.71394455 0.04713018 0.12312412] Action: 0 Reward: 1.0\n",
      "State: [ 0.79461914  0.9083607   0.04959267 -0.15432514] Action: 1 Reward: 1.0\n",
      "State: [0.81278634 0.71256506 0.04650616 0.15358153] Action: 0 Reward: 1.0\n",
      "State: [ 0.82703763  0.90699136  0.04957779 -0.12407467] Action: 1 Reward: 1.0\n",
      "State: [0.8451775  0.71119547 0.0470963  0.18382818] Action: 0 Reward: 1.0\n",
      "State: [ 0.85940135  0.905613    0.05077286 -0.09363358] Action: 1 Reward: 1.0\n",
      "State: [0.87751365 0.7098015  0.04890019 0.214626  ] Action: 0 Reward: 1.0\n",
      "State: [ 0.8917097   0.90419143  0.05319271 -0.06223948] Action: 1 Reward: 1.0\n",
      "State: [0.9097935  0.7083488  0.05194792 0.2467403 ] Action: 0 Reward: 1.0\n",
      "State: [ 0.92396045  0.90269184  0.05688273 -0.02911517] Action: 1 Reward: 1.0\n",
      "State: [0.9420143  0.70680225 0.05630042 0.2809585 ] Action: 0 Reward: 1.0\n",
      "State: [0.95615035 0.90107775 0.0619196  0.00655048] Action: 1 Reward: 1.0\n",
      "State: [ 0.9741719  1.0952595  0.0620506 -0.2659713] Action: 1 Reward: 1.0\n",
      "State: [0.99607706 0.8993094  0.05673118 0.04561945] Action: 0 Reward: 1.0\n",
      "State: [ 1.0140632   1.0935739   0.05764357 -0.2286385 ] Action: 1 Reward: 1.0\n",
      "State: [1.0359348  0.8976776  0.0530708  0.08165573] Action: 0 Reward: 1.0\n",
      "State: [ 1.0538883   1.0920002   0.05470391 -0.1938223 ] Action: 1 Reward: 1.0\n",
      "State: [1.0757283  0.89614016 0.05082747 0.11560327] Action: 0 Reward: 1.0\n",
      "State: [ 1.093651    1.0904983   0.05313953 -0.16062072] Action: 1 Reward: 1.0\n",
      "State: [1.1154611  0.8946575  0.04992712 0.14834157] Action: 0 Reward: 1.0\n",
      "State: [ 1.1333542   1.0890303   0.05289395 -0.12818179] Action: 1 Reward: 1.0\n",
      "State: [1.1551348  0.89319205 0.05033031 0.18070826] Action: 0 Reward: 1.0\n",
      "State: [ 1.1729987   1.087559    0.05394448 -0.09568213] Action: 1 Reward: 1.0\n",
      "State: [1.1947498  0.89170706 0.05203084 0.21352044] Action: 0 Reward: 1.0\n",
      "State: [ 1.212584    1.086048    0.05630124 -0.06230632] Action: 1 Reward: 1.0\n",
      "State: [1.2343049  0.8901659  0.05505512 0.24759507] Action: 0 Reward: 1.0\n",
      "State: [ 1.2521083   1.0844601   0.06000702 -0.02722665] Action: 1 Reward: 1.0\n",
      "State: [1.2737975  0.88853127 0.05946249 0.28376916] Action: 0 Reward: 1.0\n",
      "State: [1.2915682  1.082757   0.06513787 0.01041725] Action: 1 Reward: 1.0\n",
      "State: [ 1.3132232   1.2768872   0.06534622 -0.26102358] Action: 1 Reward: 1.0\n",
      "State: [1.338761   1.0808963  0.06012574 0.05153352] Action: 0 Reward: 1.0\n",
      "State: [ 1.3603789   1.2751068   0.06115641 -0.22158962] Action: 1 Reward: 1.0\n",
      "State: [1.3858811  1.0791664  0.05672462 0.08974023] Action: 0 Reward: 1.0\n",
      "State: [ 1.4074644   1.2734314   0.05851943 -0.1845204 ] Action: 1 Reward: 1.0\n",
      "State: [1.432933   1.0775231  0.05482902 0.12603395] Action: 0 Reward: 1.0\n",
      "State: [ 1.4544835   1.2718184   0.0573497  -0.14885946] Action: 1 Reward: 1.0\n",
      "State: [1.4799198  1.0759242  0.05437251 0.16135047] Action: 0 Reward: 1.0\n",
      "State: [ 1.5014384   1.2702272   0.05759952 -0.11369582] Action: 1 Reward: 1.0\n",
      "State: [1.5268428 1.0743293 0.0553256 0.1965887] Action: 0 Reward: 1.0\n",
      "State: [ 1.5483295   1.268618    0.05925738 -0.07814106] Action: 1 Reward: 1.0\n",
      "State: [1.5737019  1.0726988  0.05769455 0.23263335] Action: 0 Reward: 1.0\n",
      "State: [ 1.5951558   1.266951    0.06234722 -0.04130726] Action: 1 Reward: 1.0\n",
      "State: [1.6204948  1.070993   0.06152108 0.27037686] Action: 0 Reward: 1.0\n",
      "State: [ 1.6419147   1.2651855   0.06692861 -0.00228509] Action: 1 Reward: 1.0\n",
      "State: [1.6672183  1.0691707  0.06688291 0.31074113] Action: 0 Reward: 1.0\n",
      "State: [1.6886019  1.2632792  0.07309774 0.03987835] Action: 1 Reward: 1.0\n",
      "State: [ 1.7138674   1.457281    0.0738953  -0.22887558] Action: 1 Reward: 1.0\n",
      "State: [1.743013   1.261185   0.06931779 0.08617098] Action: 0 Reward: 1.0\n",
      "State: [ 1.7682368   1.4552485   0.07104121 -0.18386193] Action: 1 Reward: 1.0\n",
      "State: [1.7973417  1.2591857  0.06736397 0.13035892] Action: 0 Reward: 1.0\n",
      "State: [ 1.8225254   1.4532813   0.06997115 -0.14033401] Action: 1 Reward: 1.0\n",
      "State: [1.851591   1.2572305  0.06716447 0.17357717] Action: 0 Reward: 1.0\n",
      "State: [ 1.8767356   1.4513302   0.07063601 -0.09718428] Action: 1 Reward: 1.0\n",
      "State: [1.9057622  1.2552706  0.06869233 0.21692133] Action: 0 Reward: 1.0\n",
      "State: [ 1.9308677   1.4493468   0.07303075 -0.05332666] Action: 1 Reward: 1.0\n",
      "State: [1.9598546  1.2532578  0.07196422 0.2614748 ] Action: 0 Reward: 1.0\n",
      "State: [ 1.9849198   1.4472827   0.07719371 -0.00766996] Action: 1 Reward: 1.0\n",
      "State: [2.0138655  1.2511433  0.07704031 0.3083352 ] Action: 0 Reward: 1.0\n",
      "State: [2.0388882  1.4450879  0.08320702 0.04090879] Action: 1 Reward: 1.0\n",
      "State: [ 2.06779    1.6389242  0.0840252 -0.2244051] Action: 1 Reward: 1.0\n",
      "State: [2.1005685  1.4427081  0.07953709 0.09355499] Action: 0 Reward: 1.0\n",
      "State: [ 2.1294227   1.6366053   0.0814082  -0.17301206] Action: 1 Reward: 1.0\n",
      "State: [2.1621547  1.4404182  0.07794795 0.14420144] Action: 0 Reward: 1.0\n",
      "State: [ 2.190963    1.6343424   0.08083198 -0.122908  ] Action: 1 Reward: 1.0\n",
      "State: [2.22365    1.438161   0.07837382 0.19414197] Action: 0 Reward: 1.0\n",
      "State: [ 2.2524133   1.6320795   0.08225666 -0.0728245 ] Action: 1 Reward: 1.0\n",
      "State: [2.2850547  1.4358805  0.08080017 0.24463509] Action: 0 Reward: 1.0\n",
      "State: [ 2.3137724   1.629761    0.08569288 -0.02150742] Action: 1 Reward: 1.0\n",
      "State: [2.3463676  1.4335213  0.08526272 0.29693487] Action: 0 Reward: 1.0\n",
      "State: [2.3750381  1.6273309  0.09120142 0.03231348] Action: 1 Reward: 1.0\n",
      "State: [ 2.4075847   1.8210347   0.0918477  -0.23025855] Action: 1 Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "env = CustomCartPole()\n",
    "\n",
    "state, info = env.reset()\n",
    "\n",
    "for t in range(300):\n",
    "\n",
    "    if state[3] < 0:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = 1\n",
    "\n",
    "    (state, reward, done, truncated, info) = env.step(action)\n",
    "    print(f\"State: {state} Action: {action} Reward: {reward}\")\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        hidden_sizes = [256, 128, 64]\n",
    "        layers = []\n",
    "        \n",
    "        layers.append(nn.Linear(n_observations, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_sizes[-1], n_actions))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.unwrapped.theta_threshold_radians = np.deg2rad(45)\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "policy_net.load_state_dict(torch.load(\"cartpole_policy_8.pth\", map_location=torch.device('cpu'), weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  703  steps\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.unwrapped.theta_threshold_radians = np.deg2rad(45)\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=\"cpu\").unsqueeze(0)\n",
    "\n",
    "for t in count():\n",
    "    action = policy_net(state).max(1).indices.view(1, 1)\n",
    "\n",
    "    if keyboard.is_pressed(\"left\"):\n",
    "        action = torch.tensor([[0]], dtype=torch.int64)\n",
    "    elif keyboard.is_pressed(\"right\"):\n",
    "        action = torch.tensor([[1]], dtype=torch.int64)\n",
    "\n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    state = torch.tensor(observation, dtype=torch.float32, device=\"cpu\").unsqueeze(0)\n",
    "    \n",
    "    env.render()\n",
    "\n",
    "    if terminated:\n",
    "        print('Iteration: ', t+1, ' steps')\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
